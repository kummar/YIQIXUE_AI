{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN包含的卷积层和池化层，简单的说操作步骤如下：\n",
    "- CNN:\n",
    "    1. Zero Padding\n",
    "    2. Convolve Window\n",
    "    3. 向前传播Convolve \n",
    "    4. 反向传播Convolve \n",
    "- Pooling:\n",
    "    1. 向前传播Pooling\n",
    "    2. 创建MASK\n",
    "    3. 分配值\n",
    "    3. 向后传播Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PADDING的好处\n",
    "* 避免卷积的时候缩小了整体的高度和宽度，这个优势尤其体现在深度网络上。当你使用参数为'same'的时候，卷积前后的高度和宽度是不会变化的。\n",
    "* 有助于在边界保留更多有价值的信息，如果不用PADDING，下一层的少量值会被边缘的像素影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    # constant表示PADDING的方式,一般PADDING的方式就是下面这样定死的，不需要改动太多\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x.shape =', (4, 3, 3, 2))\n",
      "('x_pad.shape =', (4, 7, 7, 2))\n",
      "('x[1, 1] =', array([[ 0.90085595, -0.68372786],\n",
      "       [-0.12289023, -0.93576943],\n",
      "       [-0.26788808,  0.53035547]]))\n",
      "('x_pad[1, 1] =', array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1134503d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAACqCAYAAAAz+v3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1131db6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1, 1] =\", x[1, 1])\n",
    "print (\"x_pad[1, 1] =\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一个位置一个filter，得到一个值\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Z =', -23.160212202520778)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Convolutional Neural Networks - Forward pass\n",
    "\n",
    "在向前传播的时候，会用到多个filters对输入进行卷积。每一个'convolution'都能输出一个2D的矩阵。你需要把这几个2D的矩阵堆叠在一起形成一个3D的矩阵\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**注意用下述公式查看卷积前后的宽高变化**\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line),f是window size\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                 # loop的是训练样本的batch\n",
    "        a_prev_pad = A_prev_pad[i]                     # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Z's mean =\", 0.15585932488906465)\n",
      "('cache_conv[0][1][2][3] =', array([-0.20075807,  0.18656139,  0.41005165]))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "池化层可以减少输入的height 和 width。因此也就减少了计算量，同时，池化后拿到的特征和input的图像上的具体位置无关(位置不变性): \n",
    "\n",
    "- Max-pooling layer\n",
    "\n",
    "- Average-pooling layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "\n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "('A =', array([[[[ 1.74481176,  1.6924546 ,  2.10025514]]],\n",
      "\n",
      "\n",
      "       [[[ 1.19891788,  1.51981682,  2.18557541]]]]))\n",
      "()\n",
      "mode = average\n",
      "('A =', array([[[[-0.09498456,  0.11180064, -0.14263511]]],\n",
      "\n",
      "\n",
      "       [[[-0.09525108,  0.28325018,  0.33035185]]]]))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 4}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dA_mean =', 9.6089906758689949)\n",
      "('dW_mean =', 10.581741275547566)\n",
      "('db_mean =', 76.371069195637347)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling layer - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Max Pooling，得到mask\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = x == np.max(x)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x = ', array([[ 1.62434536, -0.61175641, -0.52817175],\n",
      "       [-1.07296862,  0.86540763, -2.3015387 ]]))\n",
      "('mask = ', array([[ True, False, False],\n",
      "       [False, False, False]], dtype=bool))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling的max位置会影响最终的输出结果，因此肯定对cost有影响，只要对cost有影响，梯度就不是0** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average Pooling\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('distributed value =', array([[ 0.,  0.],\n",
      "       [ 0.,  0.]]))\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 放入到模型中\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "('mean of dA = ', 0.14571390272918056)\n",
      "('dA_prev[1,1] = ', array([[ 0.        ,  0.        ],\n",
      "       [ 5.05844394, -1.68282702],\n",
      "       [ 0.        ,  0.        ]]))\n",
      "()\n",
      "mode = average\n",
      "('mean of dA = ', 0.14571390272918056)\n",
      "('dA_prev[1,1] = ', array([[ 0.08485462,  0.2787552 ],\n",
      "       [ 1.26461098, -0.25749373],\n",
      "       [ 1.17975636, -0.53624893]]))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
