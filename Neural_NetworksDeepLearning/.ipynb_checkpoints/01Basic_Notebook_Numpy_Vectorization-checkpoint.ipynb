{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些Python的基本操作\n",
    "\n",
    "我们先介绍一些关于Python的基本操作，这样可以方便后面的内容介绍和手动实现。\n",
    "我们主要用jupyer notebook。\n",
    "jupyer notebook 是一个非常方便的工具，可以简单的理解为“Python草稿纸”，我们在做数据挖掘和深度学习的时候，往往要进行观察数据，画图等操作。每次直接run script.py其实不是很方便，而且不能保存结果，jupyer notebook就可以完美的解决这个问题。\n",
    "你可以通过下面这个链接简单了解一下[jupyer notebook](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)\n",
    "\n",
    "**通过这个简单的Demo,你最好能掌握下面几点**\n",
    "- 学会使用jupyer notebook\n",
    "- 能用Numpy做一些简单的向量化计算\n",
    "- 明白什么是\"broadcasting\"\n",
    "- 完成一些向量化的操作，其实就是多用map少用for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyer Notebook####\n",
    "\n",
    "***如果你实在懒得看上面我给你的链接，那么你就记住下面几个快捷键吧。***\n",
    "- \"SHIFT\"+\"ENTER\" 是执行命令\n",
    "- 点击\"ESC\"再按\"Ｂ\"是在这行下面新建一行,\"A\"是在上面新建一行,\"X\"是删除这行。\n",
    "- 点击\"ESC\"再按\"Z\"是撤销上次操作。\n",
    "- 点击\"ESC\"再按\"M\"是进入MARKDOWN模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# 测试一个Print,注意Python3是要加括号的,Python2.x不需要加括号\n",
    "print \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**然后试一下自己定义一个函数**\n",
    "- 选择实现sigmoid函数\n",
    "- 我们先用math这个库,然后再用numpy，这样我们就可以看到为啥大家都用Numpy而不是其他的库了\n",
    "\n",
    "#### 我们主要实现下面两个步骤\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$ {1}\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用math定义sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2689414213699951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 完成第一个步骤\n",
    "basic_sigmoid(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-683461f067e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#　再试着完成第二个步骤,我们搞一个向量Ｘ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-36b47da5a57a>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "#　再试着完成第二个步骤,我们搞一个向量Ｘ\n",
    "X = [1, 2, 3]\n",
    "basic_sigmoid(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错了，这就是math等库的缺点，只能计算实数，而不能直接计算向量或者矩阵。\n",
    "想要正常使用就要用map或者for，讲函数用于每一个元素，这样就很麻烦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是使用for的方式:\n",
      "0.73105857863\n",
      "0.880797077978\n",
      "0.952574126822\n"
     ]
    }
   ],
   "source": [
    "print \"这是使用for的方式:\"\n",
    "for i in range(0,len(X)):\n",
    "    print basic_sigmoid(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是使用map的方式:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7310585786300049, 0.8807970779778823, 0.9525741268224334]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"这是使用map的方式:\"\n",
    "map(basic_sigmoid,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**然后我们用Numpy再进行一次尝试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用math定义sigmoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    s = 1. / (1 + np.exp(-x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2689414213699951"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 完成第一个步骤\n",
    "sigmoid(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105858,  0.88079708,  0.95257413])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　再试着完成第二个步骤,我们搞一个向量Ｘ\n",
    "X = np.array([1, 2, 3])\n",
    "sigmoid(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，不报错了，而且不需要使用for或者map，使用起来是非常方便的！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**接下来我们来学习一下求导**\n",
    "\n",
    "我们要求通过sigmoid得到的值的导，其实就是实现下试:\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "用下面两步就可以完成\n",
    "1. 计算输入的sigmoid\n",
    "2. 然后计算 $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试编sigmoid的求导函数\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1-s)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [ 0.19661193  0.10499359  0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们要学习的另一个操作是reshape**\n",
    "\n",
    "在后面学习神经网络的过程中，我们经常需要把图片数据转换成一个一维的向量(3D =>> 1D)，这个时候就需要用reshape来完成操作。\n",
    "\n",
    "图像的shape通常是: $(length, height, depth = 3)$,我们要通过reshape把它变成：(length\\*height\\*3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图片转化成向量\n",
    "def image2vector(image):\n",
    "\n",
    "    v = image.reshape(image.size, 1)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image的shape是(3, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个图片的像素(shape),实际上图片的第三个维度是３(RGB)，这里我们只是介绍其处理过程\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print \"image的shape是{}\".format(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[ 0.67826139]\n",
      " [ 0.29380381]\n",
      " [ 0.90714982]\n",
      " [ 0.52835647]\n",
      " [ 0.4215251 ]\n",
      " [ 0.45017551]\n",
      " [ 0.92814219]\n",
      " [ 0.96677647]\n",
      " [ 0.85304703]\n",
      " [ 0.52351845]\n",
      " [ 0.19981397]\n",
      " [ 0.27417313]\n",
      " [ 0.60659855]\n",
      " [ 0.00533165]\n",
      " [ 0.10820313]\n",
      " [ 0.49978937]\n",
      " [ 0.34144279]\n",
      " [ 0.94630077]]\n",
      "处理后的shape是(18, 1)\n"
     ]
    }
   ],
   "source": [
    "# 尝试使用image2vector对image的shape进行转化\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))\n",
    "print \"处理后的shape是{}\".format(image2vector(image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18就是上面公式提到的: 3 x 3 x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**另外一个操作是标准化数据**\n",
    "\n",
    "关于标准化这个东西应该不需要多讲，因为之前做机器学习的时候也是很常用的，主要用于提升收敛速度。\n",
    "\n",
    "标准化的方式也有很多，比如最大最小值标准化或者范数标准化。\n",
    "\n",
    "下面我们来实现一个范数标准化的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 4],\n",
       "       [2, 6, 4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个需要标准化的矩阵\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [2, 6, 4]])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.        ]\n",
      " [ 7.48331477]]\n"
     ]
    }
   ],
   "source": [
    "# 用Numpy求矩阵x的范数\n",
    "x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims = True)\n",
    "print x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.6       ,  0.8       ],\n",
       "       [ 0.26726124,  0.80178373,  0.53452248]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用范数标准化矩阵\n",
    "x = x / x_norm\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把上述过程整合起来定义一个范数标准化函数\n",
    "\n",
    "def normalizeRows(x):\n",
    "\n",
    "    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n",
    "    \n",
    "    x = x / x_norm\n",
    "    \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[ 0.          0.6         0.8       ]\n",
      " [ 0.26726124  0.80178373  0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [2, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**接下来我们讲讲'broadcasting'和'softmax'**\n",
    "\n",
    "broadcasting是非常重要的，而Numpy可以轻松的实现broadcasting。\n",
    "\n",
    "所谓broadcasting就是能够在不同形状的数组之间进行数值计算，如果不用Numpy，实现起来是比较复杂的。\n",
    "\n",
    "softmax这个你如果不懂的化就把它理解成某一种标准化吧，我们只用下面两个公式代表向量的softmax和矩阵的softmax\n",
    "\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "##### 看上去还是很简单的，就是先求exp,然后把exp的值加起来分别用exp除一下而已，下面我们来一起实现它，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "\n",
    "def softmax(x):\n",
    "\n",
    "    x_exp = np.exp(x)\n",
    "    \n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "        \n",
    "    # 其实这步骤是比较关键的，因为就是在这儿用到了broadcasting,只不过Numpy太方便了让你没有感知而已。\n",
    "    s = x_exp / x_sum\n",
    "    print \"不同计算阶段的shape其实是不同的:p1:{},p2:{},p3:{}\".format(x_exp.shape,x_sum.shape,s.shape)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同计算阶段的shape其实是不同的:p1:(2, 5),p2:(2, 1),p3:(2, 5)\n",
      "softmax(x) = [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
      "    1.21052389e-04]\n",
      " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
      "    8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**向量化**\n",
    "\n",
    "简单来说，向量化就是用map替代for，这样是可以节约很多时间的，能够显著的提高计算效率。\n",
    "\n",
    "因为for每次只能运算一个,而for是同是处理多个。\n",
    "\n",
    "我们结合运算时间来说明这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- 计算耗时 = 0.0748634338379ms\n",
      "outer = [[ 81.  18.  18.  81.   0.  81.  18.  45.   0.   0.  81.  18.  45.   0.\n",
      "    0.]\n",
      " [ 18.   4.   4.  18.   0.  18.   4.  10.   0.   0.  18.   4.  10.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [ 63.  14.  14.  63.   0.  63.  14.  35.   0.   0.  63.  14.  35.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [ 81.  18.  18.  81.   0.  81.  18.  45.   0.   0.  81.  18.  45.   0.\n",
      "    0.]\n",
      " [ 18.   4.   4.  18.   0.  18.   4.  10.   0.   0.  18.   4.  10.   0.\n",
      "    0.]\n",
      " [ 45.  10.  10.  45.   0.  45.  10.  25.   0.   0.  45.  10.  25.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.]]\n",
      " ----- 计算耗时 = 0.243186950684ms\n",
      "elementwise multiplication = [ 81.   4.  10.   0.   0.  63.  10.   0.   0.   0.  81.   4.  25.   0.   0.]\n",
      " ----- 计算耗时 = 0.0970363616943ms\n",
      "gdot = [ 18.79624092  25.6377409   13.43451111]\n",
      " ----- 计算耗时 = 0.133991241455ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 使用for循环将两个向量进行点乘 ###\n",
    "tic = time.time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- 计算耗时 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 使用for循环将向量和0矩阵进行点乘 ###\n",
    "tic = time.time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- 计算耗时 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 使用for循环将两个向量进行点乘 ###\n",
    "tic = time.time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- 计算耗时 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 使用for循环将两个矩阵进行点乘 ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- 计算耗时 = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0519752502441ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.0638961791992ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.0491142272949ms\n",
      "gdot = [ 18.79624092  25.6377409   13.43451111]\n",
      " ----- Computation time = 0.109910964966ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 向量化将两个向量进行点乘 ###\n",
    "tic = time.time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化将向量和0矩阵进行点乘 ###\n",
    "tic = time.time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化将两个向量进行点乘 ###\n",
    "tic = time.time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化将两个矩阵进行点乘 ###\n",
    "tic = time.time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以看到不仅时间缩短了很多，代码也简洁了很多！！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后我们来实现一下L1、L2损失函数\n",
    "\n",
    "其实就是下面两个公式，现阶段你不需要理解太深，只需要做到我们给出公式你可以用Python实现即可！\n",
    "\n",
    "- L1 loss:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$\n",
    "- L2 loss：\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "\n",
    "    loss = np.sum(np.abs((y - yhat)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# L2\n",
    "def L2(yhat, y):\n",
    "   \n",
    "    loss = np.sum(np.square(yhat - y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
